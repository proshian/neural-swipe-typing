{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# from model import get_m1_model, get_m1_bigger_model, get_m1_smaller_model\n",
    "from model import _get_transformer__vn1\n",
    "from ns_tokenizers import CharLevelTokenizerv2, KeyboardTokenizer\n",
    "from dataset import SwipeDataset, SwipeDatasetSubset\n",
    "from word_generators import GreedyGenerator, BeamGenerator, WordGenerator\n",
    "from metrics import get_mmr\n",
    "from feature_extraction.swipe_feature_extractor_factory import swipe_feature_extractor_factory\n",
    "from logit_processors import VocabularyLogitProcessor\n",
    "from model import get_transformer__from_spe_config__vn1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../data/data_preprocessed\"\n",
    "MODELS_DIR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PATH = os.path.join(DATA_ROOT, \"valid.jsonl\")\n",
    "TEST_PATH = os.path.join(DATA_ROOT, \"test.jsonl\")\n",
    "\n",
    "VOCAB_PATH = os.path.join(DATA_ROOT, \"voc.txt\")\n",
    "TOKENIZER_PATH = \"../tokenizers/keyboard/ru.json\"\n",
    "GRID_NAME_TO_GRID_PATH = os.path.join(DATA_ROOT, \"gridname_to_grid.json\")\n",
    "TRAJECTORY_FEATURES_STATISTICS_PATH = os.path.join(DATA_ROOT, \"trajectory_features_statistics.json\")\n",
    "BOUNDING_BOXES_PATH = os.path.join(DATA_ROOT, \"key_bounding_boxes.json\")\n",
    "WEIGHTED_SWIPE_FEATURE_EXTRACTOR_CONFIG_PATH = \"../configs/feature_extractor/traj_and_weights_v1.json\"\n",
    "TRAJ_AND_NEAREST_SWIPE_FEATURE_EXTRACTOR_CONFIG_PATH = \"../configs/feature_extractor/traj_and_nearest.json\"\n",
    "TRAJ_AND_NEAREST_SWIPE_POINT_EMBEDDER_CONFIG_PATH = \"../configs/swipe_point_embedder/separate_traj_and_nearest__6_coord.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_tokenizer = KeyboardTokenizer(TOKENIZER_PATH)\n",
    "subword_tokenizer = CharLevelTokenizerv2(VOCAB_PATH)\n",
    "\n",
    "data_paths = [VAL_PATH, TEST_PATH]\n",
    "\n",
    "gridname_to_grid = read_json(GRID_NAME_TO_GRID_PATH)\n",
    "trajectory_features_statistics = read_json(TRAJECTORY_FEATURES_STATISTICS_PATH)\n",
    "bounding_boxes = read_json(BOUNDING_BOXES_PATH)\n",
    "\n",
    "# grid_name_to_weighted_swipe_feature_extractor = {\n",
    "#     grid_name: swipe_feature_extractor_factory(\n",
    "#         grid=grid,\n",
    "#         keyboard_tokenizer=kb_tokenizer,\n",
    "#         trajectory_features_statistics=trajectory_features_statistics,\n",
    "#         bounding_boxes=bounding_boxes,\n",
    "#         grid_name=grid_name,\n",
    "#         component_configs=read_json(WEIGHTED_SWIPE_FEATURE_EXTRACTOR_CONFIG_PATH)\n",
    "#     )\n",
    "#     for grid_name, grid in gridname_to_grid.items()\n",
    "# }\n",
    "\n",
    "grid_name_to_traj_and_nearest_swipe_feature_extractor = {\n",
    "    grid_name: swipe_feature_extractor_factory(\n",
    "        grid=grid,\n",
    "        keyboard_tokenizer=kb_tokenizer,\n",
    "        trajectory_features_statistics=trajectory_features_statistics,\n",
    "        bounding_boxes=bounding_boxes,\n",
    "        grid_name=grid_name,\n",
    "        component_configs=read_json(TRAJ_AND_NEAREST_SWIPE_FEATURE_EXTRACTOR_CONFIG_PATH)\n",
    "    )\n",
    "    for grid_name, grid in gridname_to_grid.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 55067.57it/s]\n",
      "10000it [00:00, 41164.82it/s]\n",
      "10000it [00:00, 70957.72it/s]\n",
      "10000it [00:00, 71013.59it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = SwipeDataset(\n",
    "    data_path=VAL_PATH,\n",
    "    word_tokenizer=subword_tokenizer,\n",
    "    grid_name_to_swipe_feature_extractor=grid_name_to_traj_and_nearest_swipe_feature_extractor,\n",
    ")\n",
    "\n",
    "test_dataset = SwipeDataset(\n",
    "    data_path=TEST_PATH,\n",
    "    word_tokenizer=subword_tokenizer,\n",
    "    grid_name_to_swipe_feature_extractor=grid_name_to_traj_and_nearest_swipe_feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default_dataset = SwipeDatasetSubset(val_dataset, \"default\")\n",
    "val_extra_dataset = SwipeDatasetSubset(val_dataset, \"extra\")\n",
    "\n",
    "test_default_dataset = SwipeDatasetSubset(test_dataset, \"default\")\n",
    "test_extra_dataset = SwipeDatasetSubset(test_dataset, \"extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset: SwipeDataset, \n",
    "                subword_tokenizer: CharLevelTokenizerv2) -> List[str]:\n",
    "    targets = []\n",
    "    for _, target_tokens in dataset:\n",
    "        target = subword_tokenizer.decode(target_tokens[:-1])\n",
    "        targets.append(target)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_set(vocab_path: str):\n",
    "    with open(vocab_path, 'r', encoding = \"utf-8\") as f:\n",
    "        return set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = get_vocab_set(os.path.join(DATA_ROOT, \"voc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default_targets = get_targets(val_default_dataset, subword_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ckpt_to_pt import convert_and_save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../checkpoints/traj_and_nearest/epoch_end/v3_nearest_and_traj_transformer_bigger-default--epoch=36-val_loss=0.441-val_word_level_accuracy=0.872.ckpt\"\n",
    "weights_path = ckpt_path.replace('checkpoints', 'state_dicts').replace('.ckpt', '.pt')\n",
    "convert_and_save_file(ckpt_path, out_path=weights_path, device=device, make_dir_if_absent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zifer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GRID_NAME = \"default\"\n",
    "NUM_CLASSES = 35\n",
    "MAX_OUT_SEQ_LEN = 35\n",
    "\n",
    "\n",
    "spe_config = read_json(TRAJ_AND_NEAREST_SWIPE_POINT_EMBEDDER_CONFIG_PATH)\n",
    "\n",
    "\n",
    "\n",
    "model = get_transformer__from_spe_config__vn1(\n",
    "    spe_config=spe_config,\n",
    "    n_classes= NUM_CLASSES,\n",
    "    n_word_tokens=len(subword_tokenizer.char_to_idx),\n",
    "    max_out_seq_len=MAX_OUT_SEQ_LEN,\n",
    "    device=device,\n",
    "    weights_path=weights_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(s: str, suffix: str) -> str:\n",
    "    if s.endswith(suffix):\n",
    "        return s[:-len(suffix)]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 35\n",
    "vocab_logits_processor = VocabularyLogitProcessor(subword_tokenizer, vocab_set, max_token_id=n_classes - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAMSEARCH_NORMALIZATION_FACTOR = 0.5\n",
    "BEAMSEARCH_BEAM_SIZE = 6\n",
    "\n",
    "\n",
    "greedy_generator__no_vocab = GreedyGenerator(model, subword_tokenizer, device, max_steps_n=MAX_OUT_SEQ_LEN)\n",
    "beam_generator__no_vocab = BeamGenerator(model, subword_tokenizer, device, max_steps_n=MAX_OUT_SEQ_LEN, beamsize=BEAMSEARCH_BEAM_SIZE, normalization_factor=BEAMSEARCH_NORMALIZATION_FACTOR)\n",
    "\n",
    "greedy_generator_with_vocab = GreedyGenerator(model, subword_tokenizer, device, logit_processor=vocab_logits_processor, max_steps_n=MAX_OUT_SEQ_LEN)\n",
    "beam_generator__with_vocab = BeamGenerator(model, subword_tokenizer, device, logit_processor=vocab_logits_processor, max_steps_n=MAX_OUT_SEQ_LEN, beamsize=BEAMSEARCH_BEAM_SIZE, normalization_factor=BEAMSEARCH_NORMALIZATION_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_beamsearch(score, pred_len, normalization_factor):\n",
    "    return score * (pred_len + 1)**normalization_factor\n",
    "\n",
    "def beamsearch_score_to_prob(score, pred_len, normalization_factor):   \n",
    "    return np.exp(denormalize_beamsearch(-score, pred_len, normalization_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN = 35\n",
    "\n",
    "def predict(word_generator: WordGenerator, dataset, n_hypotheses, scores_to_prob: Callable, \n",
    "            verbose = True, n_examples = None,) -> List[List[Tuple[str, float]]]:\n",
    "    curve_id_to_hypotheses = []\n",
    "    \n",
    "    n_examples = n_examples or len(dataset)\n",
    "\n",
    "    if verbose:\n",
    "        print((\"{:<22}\" + \"{:<29}\" * n_hypotheses).format(\"target\", *[f\"pred{i}\" for i in range(1, n_hypotheses+1)]))\n",
    "        print(\"-\"*(15+30*n_hypotheses))\n",
    "\n",
    "    for i, data in enumerate(val_default_dataset):\n",
    "        if i >= n_examples:\n",
    "            return curve_id_to_hypotheses\n",
    "\n",
    "        (encoder_in, dec_in), target = data\n",
    "\n",
    "        scores_and_preds_full = word_generator(encoder_in)\n",
    "\n",
    "        preds_and_probs_full = [(pred, scores_to_prob(pred, score)) \n",
    "                                for score, pred in scores_and_preds_full]\n",
    "\n",
    "        curve_id_to_hypotheses.append(preds_and_probs_full)\n",
    "\n",
    "        true_label = subword_tokenizer.decode(target[:-1])\n",
    "\n",
    "        flat_preds_and_scores = (item for pair in preds_and_probs_full[:n_hypotheses] for item in pair)\n",
    "        if verbose:\n",
    "            print((\"{:<15}   |   \" + \"{:<16}{:.4f}   |   \" *n_hypotheses ).format(true_label, *flat_preds_and_scores))\n",
    "    \n",
    "    return curve_id_to_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_scores_from_results(swipe_id_to_hypotheses_lst: list) -> list:\n",
    "    return [[pred for pred, score in item_hypothses_lst] for item_hypothses_lst in swipe_id_to_hypotheses_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                pred1                        \n",
      "---------------------------------------------\n",
      "на                |   на              0.8943   |   \n",
      "все               |   все             0.8726   |   \n",
      "добрый            |   добрый          0.8490   |   \n",
      "девочка           |   девочка         0.8544   |   \n",
      "сказала           |   сказала         0.8611   |   \n",
      "скинь             |   скинь           0.8763   |   \n",
      "геев              |   геев            0.6401   |   \n",
      "тобой             |   тобой           0.8852   |   \n",
      "была              |   быстра          0.5474   |   \n",
      "да                |   да              0.8956   |   \n",
      "муж               |   муж             0.8123   |   \n",
      "щас               |   щас             0.9444   |   \n",
      "она               |   она             0.8963   |   \n",
      "проблема          |   проблема        0.8444   |   \n",
      "билайн            |   билайн          0.7736   |   \n",
      "уже               |   уже             0.9006   |   \n",
      "раньше            |   раньше          0.8695   |   \n",
      "рам               |   нам             0.6482   |   \n",
      "щас               |   щас             0.9426   |   \n",
      "купил             |   купил           0.8218   |   \n",
      "ты                |   ты              0.9081   |   \n",
      "зовут             |   зовут           0.8976   |   \n",
      "короче            |   короче          0.8379   |   \n",
      "размыто           |   размыто         0.4872   |   \n",
      "давай             |   давай           0.8702   |   \n",
      "отдать            |   отдать          0.4337   |   \n",
      "привет            |   привет          0.8446   |   \n",
      "не                |   не              0.8801   |   \n",
      "да                |   да              0.8980   |   \n",
      "будете            |   будете          0.8641   |   \n",
      "связи             |   связи           0.8937   |   \n",
      "колывань          |   кровать         0.3274   |   \n",
      "меня              |   меня            0.8591   |   \n",
      "напиши            |   напиши          0.8635   |   \n",
      "знаю              |   знаю            0.9120   |   \n",
      "мамой             |   мамой           0.8578   |   \n",
      "не                |   не              0.8831   |   \n",
      "ты                |   ты              0.9095   |   \n",
      "только            |   только          0.8636   |   \n",
      "они               |   они             0.8987   |   \n"
     ]
    }
   ],
   "source": [
    "curve_id_to_hypotheses = predict(\n",
    "    greedy_generator_with_vocab, val_dataset, n_hypotheses=1,\n",
    "    scores_to_prob=lambda pred, score: np.exp(-score), n_examples=N_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmr(\n",
    "    remove_scores_from_results(curve_id_to_hypotheses), \n",
    "    val_default_targets[:N_EXAMPLES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                pred1                        \n",
      "---------------------------------------------\n",
      "на                |   на              0.8725   |   \n",
      "все               |   все             0.8322   |   \n",
      "добрый            |   добрый          0.7148   |   \n",
      "девочка           |   девочка         0.6902   |   \n",
      "сказала           |   сказала         0.6949   |   \n",
      "скинь             |   скинь           0.7548   |   \n",
      "геев              |   геев            0.5662   |   \n",
      "тобой             |   тобой           0.7656   |   \n",
      "была              |   быса            0.7354   |   \n",
      "да                |   да              0.8700   |   \n",
      "муж               |   муж             0.7696   |   \n",
      "щас               |   щас             0.8363   |   \n",
      "она               |   она             0.8290   |   \n",
      "проблема          |   проблема        0.6674   |   \n",
      "билайн            |   билайн          0.6373   |   \n",
      "уже               |   уже             0.8241   |   \n",
      "раньше            |   раньше          0.7333   |   \n",
      "рам               |   нам             0.6152   |   \n",
      "щас               |   щас             0.8314   |   \n",
      "купил             |   купил           0.7371   |   \n",
      "ты                |   ты              0.8710   |   \n",
      "зовут             |   зовут           0.7650   |   \n",
      "короче            |   короче          0.7296   |   \n",
      "размыто           |   размыто         0.3807   |   \n",
      "давай             |   давай           0.7601   |   \n",
      "отдать            |   отдать          0.3639   |   \n",
      "привет            |   привет          0.7263   |   \n",
      "не                |   не              0.8577   |   \n",
      "да                |   да              0.8722   |   \n",
      "будете            |   будете          0.7277   |   \n",
      "связи             |   связи           0.7639   |   \n",
      "колывань          |   кровать         0.2382   |   \n",
      "меня              |   меня            0.7951   |   \n",
      "напиши            |   напиши          0.7245   |   \n",
      "знаю              |   знаю            0.7952   |   \n",
      "мамой             |   мамой           0.7550   |   \n",
      "не                |   не              0.8613   |   \n",
      "ты                |   ты              0.8720   |   \n",
      "только            |   только          0.7324   |   \n",
      "они               |   они             0.8323   |   \n"
     ]
    }
   ],
   "source": [
    "curve_id_to_hypotheses = predict(\n",
    "    greedy_generator__no_vocab, val_dataset, n_hypotheses=1, \n",
    "    scores_to_prob=lambda pred, score: np.exp(-score), n_examples=N_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmr(\n",
    "    remove_scores_from_results(curve_id_to_hypotheses), \n",
    "    val_default_targets[:N_EXAMPLES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                pred1                        pred2                        pred3                        pred4                        \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "на                |   на              0.8943   |   наама           0.0013   |   наособицу       0.0002   |   наощупь         0.0002   |   \n",
      "все               |   все             0.8726   |   всенародная     0.0002   |   всепожирающего  0.0001   |   все-все         0.0008   |   \n",
      "добрый            |   добрый          0.8490   |   доброй          0.0119   |   добрый-добрый   0.0013   |   добрые          0.0028   |   \n",
      "девочка           |   девочка         0.8544   |   девочки         0.0056   |   девочка-волшебница0.0003   |   девочку         0.0042   |   \n",
      "сказала           |   сказала         0.8611   |   сказал-мужик    0.0008   |   сказали         0.0027   |   сказал          0.0033   |   \n",
      "скинь             |   скинь           0.8763   |   скинь-ка        0.0013   |   скиньте         0.0013   |   скинься         0.0011   |   \n",
      "геев              |   геев            0.6401   |   гены            0.1507   |   гена            0.0679   |   генеалог        0.0022   |   \n",
      "тобой             |   тобой           0.8852   |   тобой-то        0.0012   |   тобоган         0.0015   |   тоболе          0.0012   |   \n",
      "была              |   быстра          0.5474   |   быстр           0.2372   |   быстры          0.0295   |   быстро          0.0254   |   \n",
      "да                |   да              0.8956   |   да-а-а-а        0.0013   |   даже            0.0011   |   два             0.0018   |   \n",
      "муж               |   муж             0.8123   |   мало            0.0193   |   мул             0.0119   |   мурод           0.0048   |   \n",
      "щас               |   щас             0.9444   |   шрам            0.0023   |   ща-ща           0.0012   |   щавель          0.0005   |   \n",
      "она               |   она             0.8963   |   онассис         0.0013   |   она-то          0.0012   |   они             0.0022   |   \n",
      "проблема          |   проблема        0.8444   |   проблемы        0.0041   |   проблема-то     0.0012   |   проблемам       0.0018   |   \n",
      "билайн            |   билайн          0.7736   |   билборд         0.0281   |   билборда        0.0098   |   билан           0.0111   |   \n",
      "уже               |   уже             0.9006   |   ужеобразные     0.0013   |   уже-уже         0.0014   |   ужели           0.0011   |   \n",
      "раньше            |   раньше          0.8695   |   раньше-то       0.0011   |   ранчеро         0.0006   |   ранье           0.0011   |   \n",
      "рам               |   нам             0.6482   |   рам             0.0918   |   нас             0.0480   |   пас             0.0174   |   \n",
      "щас               |   щас             0.9426   |   ща-ща           0.0014   |   щавель          0.0005   |   щавеля          0.0005   |   \n",
      "купил             |   купил           0.8218   |   купили          0.0113   |   купить          0.0079   |   курил           0.0088   |   \n",
      "ты                |   ты              0.9081   |   ты-таки         0.0002   |   ты-да           0.0004   |   тыс             0.0012   |   \n",
      "зовут             |   зовут           0.8976   |   зовут-то        0.0012   |   зовутся         0.0013   |   зовущим         0.0007   |   \n",
      "короче            |   короче          0.8379   |   корочки         0.0019   |   корочке         0.0013   |   корочек         0.0013   |   \n",
      "размыто           |   размыто         0.4872   |   размыть         0.2150   |   размытия        0.0599   |   размытие        0.0331   |   \n",
      "давай             |   давай           0.8702   |   давайся         0.0012   |   давайте         0.0012   |   давай-те        0.0007   |   \n",
      "отдать            |   отдать          0.4337   |   отжать          0.2421   |   отжевать        0.0289   |   отжечь          0.0332   |   \n",
      "привет            |   привет          0.8446   |   приветик        0.0013   |   привете         0.0013   |   привету         0.0013   |   \n",
      "не                |   не              0.8801   |   нее             0.0125   |   ненец           0.0009   |   нем             0.0014   |   \n",
      "да                |   да              0.8980   |   да-а-а-а        0.0013   |   даже            0.0012   |   дат             0.0013   |   \n",
      "будете            |   будете          0.8641   |   будет-то        0.0012   |   будешь          0.0013   |   будет           0.0020   |   \n",
      "связи             |   связи           0.8937   |   свяжи           0.0028   |   связник         0.0012   |   связист         0.0011   |   \n",
      "колывань          |   колывань        0.3531   |   кровать         0.3274   |   кроватное       0.0177   |   колыванов       0.0120   |   \n",
      "меня              |   меня            0.8591   |   меня-то         0.0012   |   менять          0.0011   |   меняя           0.0014   |   \n",
      "напиши            |   напиши          0.8635   |   напиши-ка       0.0012   |   напишите        0.0015   |   напишу          0.0029   |   \n",
      "знаю              |   знаю            0.9120   |   знаю-знаю       0.0012   |   знают           0.0032   |   знать           0.0023   |   \n",
      "мамой             |   мамой           0.8578   |   мой             0.0084   |   мамору          0.0013   |   мамочку         0.0008   |   \n",
      "не                |   не              0.8831   |   нее             0.0081   |   неинициативный  0.0001   |   ну              0.0050   |   \n",
      "ты                |   ты              0.9095   |   тымовском       0.0003   |   тымовское       0.0003   |   тымчук          0.0006   |   \n",
      "только            |   только          0.8636   |   только-то       0.0013   |   тольтеков       0.0012   |   тольятти        0.0012   |   \n",
      "они               |   они             0.8987   |   онищенко        0.0013   |   онихомикоз      0.0005   |   онигири         0.0012   |   \n"
     ]
    }
   ],
   "source": [
    "n_examples = N_EXAMPLES\n",
    "\n",
    "curve_id_to_hypotheses = predict(\n",
    "    beam_generator__with_vocab, val_dataset, n_hypotheses=4, n_examples=n_examples, \n",
    "    scores_to_prob =lambda pred, score: beamsearch_score_to_prob(score, len(pred) + 1, BEAMSEARCH_NORMALIZATION_FACTOR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmr(\n",
    "    remove_scores_from_results(curve_id_to_hypotheses), \n",
    "    val_default_targets[:N_EXAMPLES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                pred1                        pred2                        pred3                        pred4                        \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "на                |   на              0.8725   |   нас             0.0014   |   нам             0.0013   |   наа             0.0012   |   \n",
      "все               |   все             0.8322   |   всем            0.0012   |   все-            0.0012   |   всео            0.0012   |   \n",
      "добрый            |   добрый          0.7148   |   доброй          0.0102   |   добрые          0.0024   |   добрый-         0.0010   |   \n",
      "девочка           |   девочка         0.6902   |   девочки         0.0045   |   девочку         0.0033   |   девочкам        0.0015   |   \n",
      "сказала           |   сказала         0.6949   |   сказали         0.0022   |   сказал          0.0028   |   сказалаща       0.0007   |   \n",
      "скинь             |   скинь           0.7548   |   скинь-ки        0.0005   |   скино           0.0017   |   скинья          0.0011   |   \n",
      "геев              |   геев            0.5662   |   гены            0.1368   |   гена            0.0611   |   генев           0.0139   |   \n",
      "тобой             |   тобой           0.7656   |   тобойь          0.0011   |   тобойа          0.0010   |   тобойн          0.0010   |   \n",
      "была              |   быса            0.7354   |   бысан           0.0085   |   баса            0.0117   |   бысам           0.0079   |   \n",
      "да                |   да              0.8700   |   да-а-а-а        0.0009   |   два             0.0016   |   да-а-а-а-       0.0001   |   \n",
      "муж               |   муж             0.7696   |   мало            0.0183   |   мул             0.0112   |   мужа            0.0052   |   \n",
      "щас               |   щас             0.8363   |   щаса            0.0015   |   щаст            0.0012   |   щаси            0.0012   |   \n",
      "она               |   она             0.8290   |   онат            0.0012   |   они             0.0021   |   онао            0.0011   |   \n",
      "проблема          |   проблема        0.6674   |   проблемы        0.0032   |   проблемам       0.0014   |   проблемаа       0.0011   |   \n",
      "билайн            |   билайн          0.6373   |   биллайн         0.0075   |   билан           0.0091   |   билбацев        0.0031   |   \n",
      "уже               |   уже             0.8241   |   ужел            0.0012   |   ужеж            0.0012   |   уже-            0.0012   |   \n",
      "раньше            |   раньше          0.7333   |   раньшей         0.0017   |   раньшее         0.0012   |   раньште         0.0012   |   \n",
      "рам               |   нам             0.6152   |   рам             0.0874   |   нас             0.0461   |   пас             0.0168   |   \n",
      "щас               |   щас             0.8314   |   щаса            0.0015   |   щась            0.0012   |   щасы            0.0012   |   \n",
      "купил             |   купил           0.7371   |   купили          0.0097   |   купить          0.0067   |   курил           0.0081   |   \n",
      "ты                |   ты              0.8710   |   тыу             0.0012   |   тыц             0.0011   |   тыя             0.0011   |   \n",
      "зовут             |   зовут           0.7650   |   зовуту          0.0012   |   зовутэ          0.0011   |   зовуте          0.0011   |   \n",
      "короче            |   короче          0.7296   |   корочки         0.0016   |   корочей         0.0012   |   корочем         0.0011   |   \n",
      "размыто           |   размыто         0.3807   |   размыть         0.1697   |   размыти         0.0707   |   размыта         0.0077   |   \n",
      "давай             |   давай           0.7601   |   давайэ          0.0011   |   давайд          0.0010   |   давайх          0.0010   |   \n",
      "отдать            |   отдать          0.3639   |   отжать          0.1357   |   отжрать         0.0644   |   отжело          0.0236   |   \n",
      "привет            |   привет          0.7263   |   приветик        0.0010   |   привете         0.0011   |   привету         0.0011   |   \n",
      "не                |   не              0.8577   |   нее             0.0117   |   нем             0.0013   |   нен             0.0013   |   \n",
      "да                |   да              0.8722   |   да-а-а-а        0.0009   |   дат             0.0012   |   дал             0.0012   |   \n",
      "будете            |   будете          0.7277   |   будетей         0.0013   |   будетев         0.0012   |   будетет         0.0011   |   \n",
      "связи             |   связи           0.7639   |   свяжи           0.0023   |   связит          0.0012   |   связин          0.0012   |   \n",
      "колывань          |   кровать         0.2382   |   колывает        0.1276   |   колываю         0.0558   |   колынь          0.0294   |   \n",
      "меня              |   меня            0.7951   |   меняя           0.0012   |   меняо           0.0011   |   меняз           0.0011   |   \n",
      "напиши            |   напиши          0.7245   |   напишу          0.0024   |   напишите        0.0011   |   напишио         0.0011   |   \n",
      "знаю              |   знаю            0.7952   |   знают           0.0027   |   знать           0.0020   |   знаюь           0.0011   |   \n",
      "мамой             |   мамой           0.7550   |   матой           0.0048   |   мамой-то        0.0009   |   мамойт          0.0012   |   \n",
      "не                |   не              0.8613   |   нее             0.0076   |   ну              0.0048   |   неге            0.0010   |   \n",
      "ты                |   ты              0.8720   |   тыи             0.0012   |   тыо             0.0012   |   тых             0.0012   |   \n",
      "только            |   только          0.7324   |   толькоо         0.0011   |   тольков         0.0011   |   толькоь         0.0011   |   \n",
      "они               |   они             0.8323   |   она             0.0025   |   онин            0.0013   |   оним            0.0013   |   \n"
     ]
    }
   ],
   "source": [
    "n_examples = N_EXAMPLES\n",
    "\n",
    "curve_id_to_hypotheses = predict(\n",
    "    beam_generator__no_vocab, val_dataset, n_hypotheses=4, n_examples=n_examples,\n",
    "    scores_to_prob =lambda pred, score: beamsearch_score_to_prob(score, len(pred) + 1, BEAMSEARCH_NORMALIZATION_FACTOR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmr(\n",
    "    remove_scores_from_results(curve_id_to_hypotheses), \n",
    "    val_default_targets[:N_EXAMPLES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('на', np.float64(0.8725337144269064)), ('нас', np.float64(0.0013806192692899187)), ('нам', np.float64(0.0012561376235014612)), ('наа', np.float64(0.0012083352994650847)), ('нан', np.float64(0.0012065850960813914)), ('нао', np.float64(0.0011999743315939924))]\n",
      "\n",
      "[('все', np.float64(0.8322106148932569)), ('всем', np.float64(0.0011985692692927101)), ('все-', np.float64(0.0011786678999622794)), ('всео', np.float64(0.0011669255543384498)), ('всеп', np.float64(0.0011558147330768588)), ('всен', np.float64(0.0011325914587499237))]\n"
     ]
    }
   ],
   "source": [
    "print(curve_id_to_hypotheses[0][:6])\n",
    "print()\n",
    "print(curve_id_to_hypotheses[1][:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Как и ожидается, после денормализации бимсерча вероятности не такие же как в greedy\n",
    "\n",
    "Нужно отметить, что в общем случае должно быть так:\n",
    "* greedy_search_no_vocab__results == beamsearch__no_vocab__results\n",
    "* greedy_search_with_voc__results == beamsearch__with_voc__results\n",
    "* greedy_search_with_voc__results != beamsearch__no_vocab__results\n",
    "* greedy_search__no_vocab__results != beamsearch__with_voc__results\n",
    "\n",
    "\n",
    "\n",
    "Некоторые заметки:\n",
    "1. Иногда вероятность оказывается немонотонной. Это именно из-за того, что мы убрали нормализацию бимсерча. То есть убрали штраф за краткость, а при ранжировании он был\n",
    "2. Даже если beamsize = n_classes, beamsearch не оценивает явно вероятность всех слов, потому что из-за over confidence вероятности некоторых возможных токенов оказываются нулевыми и эта ветка обрывается\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Batched Greedy Search example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched greedy search Pros:\n",
    "* produces the same results as unbatched variant (make sure to compare with no_vocab version)\n",
    "* works faster\n",
    "Batched greedy search Cons:\n",
    "* doesn't support vocab masking yet\n",
    "* has a different interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import CollateFn\n",
    "from word_generators import GreedyGeneratorBatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = CollateFn(batch_first=False, word_pad_idx=subword_tokenizer.char_to_idx['<pad>'])\n",
    "val_default_dataloader = DataLoader(val_default_dataset, batch_size=N_EXAMPLES, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_generator_batched__no_vocab = GreedyGeneratorBatched(model, subword_tokenizer, device, max_steps_n=MAX_OUT_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN = 35\n",
    "\n",
    "\n",
    "def remove_tokens(tensor, tokens_to_remove):\n",
    "    device = tensor.device\n",
    "    return tensor[~torch.isin(tensor, torch.tensor(tokens_to_remove, device=device))]\n",
    "\n",
    "def predict_batched(word_generator: WordGenerator, dataloader, max_steps: int = MAX_WORD_LEN, \n",
    "                    verbose: bool = True, n_batches: int = None \n",
    "                    ):\n",
    "    n_hypotheses = 1\n",
    "\n",
    "    pad_token_id = word_generator.tokenizer.char_to_idx['<pad>']\n",
    "    eos_token_id = word_generator.tokenizer.char_to_idx['<eos>']\n",
    "    sos_token_id = word_generator.tokenizer.char_to_idx['<sos>']\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print((\"{:<22}\" + \"{:<29}\" * n_hypotheses).format(\"target\", *[f\"pred{i}\" for i in range(1, n_hypotheses+1)]))\n",
    "        print(\"-\"*(15+30*n_hypotheses))\n",
    "\n",
    "    for i, ((encoder_in, _, encoder_pad_mask, _), target) in enumerate(dataloader):\n",
    "        target = target.T\n",
    "        if n_batches is not None and i >= n_batches:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            char_token_ids, log_probs = word_generator(encoder_in, encoder_pad_mask)\n",
    "            char_token_ids = char_token_ids.T\n",
    "\n",
    "            pred_words = [subword_tokenizer.decode(remove_tokens(char_token_ids[i], [pad_token_id, eos_token_id, sos_token_id])) \n",
    "                          for i in range(char_token_ids.size(0))]\n",
    "            target_words = [subword_tokenizer.decode(remove_tokens(target[i], [pad_token_id, eos_token_id, sos_token_id]))\n",
    "                            for i in range(target.size(0))]\n",
    "            log_probs = log_probs.cpu().numpy()\n",
    "            probs = np.exp(log_probs)\n",
    "\n",
    "            if verbose:\n",
    "                for true_label, pred_word, log_prob in zip(target_words, pred_words, probs):\n",
    "                    print((\"{:<15}   |   \" + \"{:<16}{:.4f}   |   \" *n_hypotheses ).format(true_label, pred_word, log_prob))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target                pred1                        \n",
      "---------------------------------------------\n",
      "на                |   на              0.8725   |   \n",
      "все               |   все             0.8322   |   \n",
      "добрый            |   добрый          0.7148   |   \n",
      "девочка           |   девочка         0.6902   |   \n",
      "сказала           |   сказала         0.6949   |   \n",
      "скинь             |   скинь           0.7548   |   \n",
      "геев              |   геев            0.5662   |   \n",
      "тобой             |   тобой           0.7656   |   \n",
      "была              |   быса            0.7354   |   \n",
      "да                |   да              0.8700   |   \n",
      "муж               |   муж             0.7696   |   \n",
      "щас               |   щас             0.8363   |   \n",
      "она               |   она             0.8290   |   \n",
      "проблема          |   проблема        0.6674   |   \n",
      "билайн            |   билайн          0.6373   |   \n",
      "уже               |   уже             0.8241   |   \n",
      "раньше            |   раньше          0.7333   |   \n",
      "рам               |   нам             0.6152   |   \n",
      "щас               |   щас             0.8314   |   \n",
      "купил             |   купил           0.7371   |   \n",
      "ты                |   ты              0.8710   |   \n",
      "зовут             |   зовут           0.7650   |   \n",
      "короче            |   короче          0.7296   |   \n",
      "размыто           |   размыто         0.3807   |   \n",
      "давай             |   давай           0.7601   |   \n",
      "отдать            |   отдать          0.3639   |   \n",
      "привет            |   привет          0.7263   |   \n",
      "не                |   не              0.8577   |   \n",
      "да                |   да              0.8722   |   \n",
      "будете            |   будете          0.7277   |   \n",
      "связи             |   связи           0.7639   |   \n",
      "колывань          |   кровать         0.2382   |   \n",
      "меня              |   меня            0.7951   |   \n",
      "напиши            |   напиши          0.7245   |   \n",
      "знаю              |   знаю            0.7952   |   \n",
      "мамой             |   мамой           0.7550   |   \n",
      "не                |   не              0.8613   |   \n",
      "ты                |   ты              0.8720   |   \n",
      "только            |   только          0.7324   |   \n",
      "они               |   они             0.8323   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zifer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict_batched(greedy_generator_batched__no_vocab, val_default_dataloader, n_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
